# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains a large assortment of data including job titles, loan usage, months, days, housing information, and more. We are seeking to predict a determination of the individual's 'success' via these factors which can be seen in the following line of code in our training file: 

"x_df["poutcome"] = x_df.poutcome.apply(lambda s: 1 if s == "success" else 0)

y_df = x_df.pop("y").apply(lambda s: 1 if s == "yes" else 0)"

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
In our experiment we tested the sample data with a Random Sampling as our parameter sampler which we determined was most effective for this experiment. You can view this in the following code:

"ps = RandomParameterSampling( {
    "learning_rate": normal(10, 3),
    "keep_probability": uniform(0.05, 0.1),
    "batch_size": choice(16, 32, 64, 128)
})"

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The pipeline's architecture consisted of creation of a objects to store our datasets, utilizing that dataset in two seperated strips of test and train, creating evaluations to determine success, ensure the pipeline has proper policies and parameter sampling imported to ensure proper testing, and then running the model to receive our classification algorithm.


**What are the benefits of the parameter sampler you chose?**
Random Sampling is typically implemented due to the fact that it is the quickest, and performs similarly in accuracy with other models such as Grid Sampling.

**What are the benefits of the early stopping policy you chose?**
I implemented a Bandit Policy as the three factors provided with implementing this policy (evaluation interval, delay evaluation, and slack factor) allow us to run experiments without having to stress over unneccessary experiments that run for a long period of time only to come out unneeded.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The AutoML Configuration drew a similar result to ours generated by manual implementation, but with slight increase in more accurate hyperparamter tuning.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
There was no major differences noticeable between the two, but the AutoML had performed rather quickly with a slight advantage in hyperparamter tuning.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
I would test changing some factored information such as increasing the test value from 40% to 60% and train value from 60% to 40% to see if the model is ready to go into production (after having run similar experiments to ensure similar results are being drawn each time.)
